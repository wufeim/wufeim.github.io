<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta name="google-site-verification" content="nnOcJxhf3tCKBL4Jm8iONLUhhtoEPtFYLwoke73tD1c"> <meta http-equiv="Permissions-Policy" content="interest-cohort=()"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> publications | Wufei Ma </title> <meta name="author" content="Wufei Ma"> <meta name="description" content="publications in reversed chronological order"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://wufeim.github.io/publications/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav sticky-bottom-footer"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Wufei</span> Ma </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/publications/">publications <span class="sr-only">(current)</span> </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">publications</h1> <p class="post-description">publications in reversed chronological order</p> </header> <article> <div class="publications"> <h2 class="bibliography">2025</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-3 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/dinemo-480.webp 480w,/assets/img/publication_preview/dinemo-800.webp 800w,/assets/img/publication_preview/dinemo-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/dinemo.jpg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="dinemo.jpg" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="wang2024compositional" class="col-sm-9"> <div class="title">DINeMo: Learning Neural Mesh Models with no 3D Annotations</div> <div class="author"> Weijie Guo, <a href="https://openreview.net/profile?id=~Guofeng_Zhang4" rel="external nofollow noopener" target="_blank">Guofeng Zhang</a>, <em>Wufei Ma</em>, and <a href="https://www.cs.jhu.edu/~ayuille/" rel="external nofollow noopener" target="_blank">Alan Yuille</a> </div> <div class="periodical"> <em>arXiv preprint</em>, 2025 </div> <div class="periodical"> </div> <div> <abbr class="badge rounded" style="background-color:#00369f"> 3D Vision </abbr> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://analysis-by-synthesis.github.io/DINeMo/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Webpage</a> <a href="http://arxiv.org/abs/2503.20220" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> </div> <div class="abstract hidden"> <p>We present DINeMo, a novel neural mesh model that is trained with no 3D annotations by leveraging pseudo-correspondence obtained from large visual foundation models.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-3 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/3dvlm-teaser2-2-480.webp 480w,/assets/img/publication_preview/3dvlm-teaser2-2-800.webp 800w,/assets/img/publication_preview/3dvlm-teaser2-2-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/3dvlm-teaser2-2.jpg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="3dvlm-teaser2-2.jpg" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="ma2025spatialllm" class="col-sm-9"> <div class="title">SpatialLLM: A Compound 3D-Informed Design towards Spatially-Intelligent Large Multimodal Models</div> <div class="author"> <em>Wufei Ma</em>, Luoxin Ye, Celso M de Melo, <a href="https://www.cs.jhu.edu/~ayuille/" rel="external nofollow noopener" target="_blank">Alan Yuille</a>, and <a href="https://beckschen.github.io" rel="external nofollow noopener" target="_blank">Jieneng Chen</a> </div> <div class="periodical"> <em>In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em> , 2025 </div> <div class="periodical"> </div> <div> <abbr class="badge rounded" style="background-color:#00369f"> 3D Vision </abbr> <abbr class="badge rounded" style="background-color:#b509ac"> Vision-Lanugage </abbr> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://3d-spatial-reasoning.github.io/spatial-llm/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Webpage</a> </div> <div class="abstract hidden"> <p>We systematically study the impact of 3D-informed data, architecture, and training setups and present SpatialLLM, an LMM with advanced 3D spatial reasoning abilities.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-3 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/pulsecheck-480.webp 480w,/assets/img/publication_preview/pulsecheck-800.webp 800w,/assets/img/publication_preview/pulsecheck-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/pulsecheck.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="pulsecheck.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="wang2025pulsecheck457" class="col-sm-9"> <div class="title">PulseCheck457: A Diagnostic Benchmark for Comprehensive Spatial Reasoning of Large Multimodal Models</div> <div class="author"> <a href="https://xingruiwang.github.io" rel="external nofollow noopener" target="_blank">Xingrui Wang</a>, <em>Wufei Ma</em> , Tiezheng Zhang, Celso M de Melo, <a href="https://beckschen.github.io" rel="external nofollow noopener" target="_blank">Jieneng Chen</a>, and <a href="https://www.cs.jhu.edu/~ayuille/" rel="external nofollow noopener" target="_blank">Alan Yuille</a> </div> <div class="periodical"> <em>In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em> , 2025 </div> <div class="periodical"> </div> <div> <abbr class="badge rounded" style="background-color:#009f36"> Dataset </abbr> <abbr class="badge rounded" style="background-color:#00369f"> 3D Vision </abbr> <abbr class="badge rounded" style="background-color:#b509ac"> Vision-Lanugage </abbr> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2502.08636" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> </div> <div class="abstract hidden"> <p>We present PulseCheck457, a scalable and unbiased synthetic dataset designed with 4 key capability for spatial reasoning.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-3 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/superclevr_phy-480.webp 480w,/assets/img/publication_preview/superclevr_phy-800.webp 800w,/assets/img/publication_preview/superclevr_phy-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/superclevr_phy.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="superclevr_phy.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="wang2024compositionam" class="col-sm-9"> <div class="title">Compositional 4D Dynamic Scenes Understanding with Physics Priors for Video Question Answering</div> <div class="author"> <a href="https://xingruiwang.github.io" rel="external nofollow noopener" target="_blank">Xingrui Wang</a>, <em>Wufei Ma</em> , <a href="https://angtianwang.github.io" rel="external nofollow noopener" target="_blank">Angtian Wang</a> , Shuo Chen, <a href="https://adamkortylewski.com" rel="external nofollow noopener" target="_blank">Adam Kortylewski</a>, and <a href="https://www.cs.jhu.edu/~ayuille/" rel="external nofollow noopener" target="_blank">Alan Yuille</a> </div> <div class="periodical"> <em>In The Thirteenth International Conference on Learning Representations</em> , 2025 </div> <div class="periodical"> </div> <div> <abbr class="badge rounded" style="background-color:#009f36"> Dataset </abbr> <abbr class="badge rounded" style="background-color:#00369f"> 3D Vision </abbr> <abbr class="badge rounded" style="background-color:#b509ac"> Vision-Lanugage </abbr> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2406.00622" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> </div> <div class="abstract hidden"> <p>We develop a video question answering dataset and study the dynamics properties of objects.</p> </div> </div> </div> </li> </ol> <h2 class="bibliography">2024</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-3 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/3dsrbench-480.webp 480w,/assets/img/publication_preview/3dsrbench-800.webp 800w,/assets/img/publication_preview/3dsrbench-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/3dsrbench.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="3dsrbench.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="wang2024compositionan" class="col-sm-9"> <div class="title">3DSRBench: A Comprehensive 3D Spatial Reasoning Benchmark</div> <div class="author"> <em>Wufei Ma</em> , Haoyu Chen, <a href="https://openreview.net/profile?id=~Guofeng_Zhang4" rel="external nofollow noopener" target="_blank">Guofeng Zhang</a>, Celso M de Melo, <a href="https://www.cs.jhu.edu/~ayuille/" rel="external nofollow noopener" target="_blank">Alan Yuille</a>, and <a href="https://beckschen.github.io" rel="external nofollow noopener" target="_blank">Jieneng Chen</a> </div> <div class="periodical"> <em>arXiv preprint</em>, 2024 </div> <div class="periodical"> </div> <div> <abbr class="badge rounded" style="background-color:#00369f"> 3D Vision </abbr> <abbr class="badge rounded" style="background-color:#b509ac"> Vision-Lanugage </abbr> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://3dsrbench.github.io" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Webpage</a> <a href="http://arxiv.org/abs/2412.07825" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> </div> <div class="abstract hidden"> <p>We present 3DSRBench, a comprehensive 3D spatial reasoning benchmark.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-3 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/imagenet3d_logo-480.webp 480w,/assets/img/publication_preview/imagenet3d_logo-800.webp 800w,/assets/img/publication_preview/imagenet3d_logo-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/imagenet3d_logo.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="imagenet3d_logo.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="ma2024imagenet3d" class="col-sm-9"> <div class="title">ImageNet3D: Towards General-Purpose Object-Level 3D Understanding</div> <div class="author"> <em>Wufei Ma</em>, <a href="https://openreview.net/profile?id=~Guofeng_Zhang4" rel="external nofollow noopener" target="_blank">Guofeng Zhang</a>, <a href="https://qihao067.github.io" rel="external nofollow noopener" target="_blank">Qihao Liu</a>, <a href="https://scholar.google.com/citations?user=SU6ooAQAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Guanning Zeng</a>, <a href="https://adamkortylewski.com" rel="external nofollow noopener" target="_blank">Adam Kortylewski</a> , <a href="https://www.cs.jhu.edu/~yyliu/" rel="external nofollow noopener" target="_blank">Yaoyao Liu</a>, and <a href="https://www.cs.jhu.edu/~ayuille/" rel="external nofollow noopener" target="_blank">Alan Yuille</a> </div> <div class="periodical"> <em>In Advances in Neural Information Processing Systems</em> , 2024 </div> <div class="periodical"> </div> <div> <abbr class="badge rounded" style="background-color:#009f36"> Dataset </abbr> <abbr class="badge rounded" style="background-color:#00369f"> 3D Vision </abbr> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://imagenet3d.github.io" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Webpage</a> <a href="http://arxiv.org/abs/2406.09613" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="https://github.com/wufeim/imagenet3d" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Data</a> <a href="https://github.com/wufeim/imagenet3d_exp" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>We present ImageNet3D, a large dataset for general-purpose object-level 3D understanding.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-3 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/feint6k-480.webp 480w,/assets/img/publication_preview/feint6k-800.webp 800w,/assets/img/publication_preview/feint6k-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/feint6k.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="feint6k.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="ma2024rethinking" class="col-sm-9"> <div class="title">Rethinking Video-Text Understanding: Retrieval from Counterfactually Augmented Data</div> <div class="author"> <em>Wufei Ma</em>, <a href="https://sites.google.com/view/kaisqu/" rel="external nofollow noopener" target="_blank">Kai Li</a>, <a href="https://scholar.google.com/citations?user=h8bGMF4AAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Zhongshi Jiang</a>, <a href="http://www.cs.umd.edu/~mmeshry/" rel="external nofollow noopener" target="_blank">Moustafa Meshry</a>, <a href="https://qihao067.github.io" rel="external nofollow noopener" target="_blank">Qihao Liu</a>, <a href="https://csrhddlam.github.io" rel="external nofollow noopener" target="_blank">Huiyu Wang</a>, <a href="https://scholar.google.com/citations?user=AliuYd0AAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Christian Häne</a>, and <a href="https://www.cs.jhu.edu/~ayuille/" rel="external nofollow noopener" target="_blank">Alan Yuille</a> </div> <div class="periodical"> <em>In European Conference on Computer Vision</em> , 2024 </div> <div class="periodical"> </div> <div class="periodical"> <span style="color:#e74c3c"><b>(Strong Double Blind)</b></span> </div> <div> <abbr class="badge rounded" style="background-color:#009f36"> Dataset </abbr> <abbr class="badge rounded" style="background-color:#b509ac"> Vision-Lanugage </abbr> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://feint6k.github.io" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Webpage</a> <a href="http://arxiv.org/abs/2407.13094" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="https://github.com/facebookresearch/feint6k" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Data</a> </div> <div class="abstract hidden"> <p>We propose a novel task, retrieval from counterfacually augmented data, and a dataset, Feint6K, for video-text understanding.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-3 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/rcnet-480.webp 480w,/assets/img/publication_preview/rcnet-800.webp 800w,/assets/img/publication_preview/rcnet-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/rcnet.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="rcnet.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="jesslen2024novum" class="col-sm-9"> <div class="title">NOVUM: Neural Object Volumes for Robust Object Classification</div> <div class="author"> <a href="https://artur.jesslen.ch" rel="external nofollow noopener" target="_blank">Artur Jesslen</a>, <a href="https://openreview.net/profile?id=~Guofeng_Zhang4" rel="external nofollow noopener" target="_blank">Guofeng Zhang</a> , <a href="https://angtianwang.github.io" rel="external nofollow noopener" target="_blank">Angtian Wang</a>, <em>Wufei Ma</em>, <a href="https://www.cs.jhu.edu/~ayuille/" rel="external nofollow noopener" target="_blank">Alan Yuille</a>, and <a href="https://adamkortylewski.com" rel="external nofollow noopener" target="_blank">Adam Kortylewski</a> </div> <div class="periodical"> <em>In European Conference on Computer Vision</em> , 2024 </div> <div class="periodical"> </div> <div> <abbr class="badge rounded" style="background-color:#00369f"> 3D Vision </abbr> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2305.14668" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> </div> <div class="abstract hidden"> <p>We introduce NOVUM, a 3D compositional model for 3D-aware image classification.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-3 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/tmm_eg-480.webp 480w,/assets/img/publication_preview/tmm_eg-800.webp 800w,/assets/img/publication_preview/tmm_eg-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/tmm_eg.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="tmm_eg.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="ma2024uncertainty" class="col-sm-9"> <div class="title">Uncertainty-Aware Deep Video Compression with Ensembles</div> <div class="author"> <em>Wufei Ma</em> , <a href="https://www.microsoft.com/en-us/research/people/jiahali/" rel="external nofollow noopener" target="_blank">Jiahao Li</a> , <a href="https://www.microsoft.com/en-us/research/people/libin/" rel="external nofollow noopener" target="_blank">Bin Li</a>, and <a href="https://www.microsoft.com/en-us/research/people/yanlu/" rel="external nofollow noopener" target="_blank">Yan Lu</a> </div> <div class="periodical"> <em>IEEE Transactions on Multimedia</em>, 2024 </div> <div class="periodical"> </div> <div> <abbr class="badge rounded" style="background-color:#d35400"> Compression </abbr> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2403.19158" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="" class="btn btn-sm z-depth-0" role="button">Journal</a> </div> <div class="abstract hidden"> <p>We propose an uncertainty-aware video compression model that effectively captures the predictive uncertainty with deep ensembles and saves bits by more than 20% when compared to DVC Pro.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-3 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/3ddst_compressed-480.webp 480w,/assets/img/publication_preview/3ddst_compressed-800.webp 800w,/assets/img/publication_preview/3ddst_compressed-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/3ddst_compressed.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="3ddst_compressed.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="ma2024generating" class="col-sm-9"> <div class="title">Generating Images with 3D Annotations Using Diffusion Models</div> <div class="author"> <em>Wufei Ma<sup>*</sup></em>, <a href="https://qihao067.github.io" rel="external nofollow noopener" target="_blank">Qihao Liu<sup>*</sup></a> , <a href="https://jiahaoplus.github.io" rel="external nofollow noopener" target="_blank">Jiahao Wang<sup>*</sup></a> , <a href="https://angtianwang.github.io" rel="external nofollow noopener" target="_blank">Angtian Wang</a>, <a href="https://www.xiaodingyuan.com" rel="external nofollow noopener" target="_blank">Xiaoding Yuan</a> , Yi Zhang, <a href="https://scholar.google.com/citations?user=ucb6UssAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Zihao Xiao</a>, <a href="https://openreview.net/profile?id=~Guofeng_Zhang4" rel="external nofollow noopener" target="_blank">Guofeng Zhang</a> , Beijia Lu, Ruxiao Duan, Yongrui Qi, <a href="https://adamkortylewski.com" rel="external nofollow noopener" target="_blank">Adam Kortylewski</a> , <a href="https://www.cs.jhu.edu/~yyliu/" rel="external nofollow noopener" target="_blank">Yaoyao Liu</a>, and <a href="https://www.cs.jhu.edu/~ayuille/" rel="external nofollow noopener" target="_blank">Alan Yuille</a> <i class="fa-solid fa-circle-info ml-1" data-toggle="popover" data-placement="top" data-html="true" data-content="* equal contribution"> </i> </div> <div class="periodical"> <em>In The Twelfth International Conference on Learning Representations</em> , 2024 </div> <div class="periodical"> </div> <div class="periodical"> <span style="color:#e74c3c"><b>(Spotlight, 5%)</b></span> </div> <div> <abbr class="badge rounded" style="background-color:#009f36"> Dataset </abbr> <abbr class="badge rounded" style="background-color:#00369f"> 3D Vision </abbr> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2306.08103" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="https://github.com/wufeim/DST3D" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>We propose 3D-DST that generates synthetic data with 3D groundtruth by incorporating 3D geomeotry control into diffusion models. With our diverse prompt generation, we effectively improve both in-distribution (ID) and out-of-distribution (OOD) performance for various 2D and 3D vision tasks.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-3 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/deformable_nemo-480.webp 480w,/assets/img/publication_preview/deformable_nemo-800.webp 800w,/assets/img/publication_preview/deformable_nemo-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/deformable_nemo.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="deformable_nemo.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="wang2024neural" class="col-sm-9"> <div class="title">Neural textured deformable meshes for robust analysis-by-synthesis</div> <div class="author"> <a href="https://angtianwang.github.io" rel="external nofollow noopener" target="_blank">Angtian Wang<sup>*</sup></a>, <em>Wufei Ma<sup>*</sup></em>, <a href="https://www.cs.jhu.edu/~ayuille/" rel="external nofollow noopener" target="_blank">Alan Yuille</a>, and <a href="https://adamkortylewski.com" rel="external nofollow noopener" target="_blank">Adam Kortylewski</a> <i class="fa-solid fa-circle-info ml-1" data-toggle="popover" data-placement="top" data-html="true" data-content="* equal contribution"> </i> </div> <div class="periodical"> <em>In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</em> , 2024 </div> <div class="periodical"> </div> <div> <abbr class="badge rounded" style="background-color:#00369f"> 3D Vision </abbr> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2306.00118" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> </div> <div class="abstract hidden"> <p>We introduce Neural Textured Deformable Meshes (NTDM), which learns a neural mesh model with deformable geometry and enables optimization on both camera parameters and object geometries.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-3 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/synthetic_nemo-480.webp 480w,/assets/img/publication_preview/synthetic_nemo-800.webp 800w,/assets/img/publication_preview/synthetic_nemo-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/synthetic_nemo.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="synthetic_nemo.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="yang2024robust" class="col-sm-9"> <div class="title">Robust Category-Level 3D Pose Estimation from Diffusion-Enhanced Synthetic Data</div> <div class="author"> <a href="https://www.linkedin.com/in/jiahao-yang-sd/" rel="external nofollow noopener" target="_blank">Jiahao Yang</a>, <em>Wufei Ma</em> , <a href="https://angtianwang.github.io" rel="external nofollow noopener" target="_blank">Angtian Wang</a>, <a href="https://www.xiaodingyuan.com" rel="external nofollow noopener" target="_blank">Xiaoding Yuan</a>, <a href="https://www.cs.jhu.edu/~ayuille/" rel="external nofollow noopener" target="_blank">Alan Yuille</a>, and <a href="https://adamkortylewski.com" rel="external nofollow noopener" target="_blank">Adam Kortylewski</a> </div> <div class="periodical"> <em>In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</em> , 2024 </div> <div class="periodical"> </div> <div> <abbr class="badge rounded" style="background-color:#00369f"> 3D Vision </abbr> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2305.16124" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> </div> <div class="abstract hidden"> <p>We introduce SyntheticP3D, a synthetic dataset for object pose estimation, and CC3D that adapts neural mesh models from synthetic to real data.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-3 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/oodcv_v2-480.webp 480w,/assets/img/publication_preview/oodcv_v2-800.webp 800w,/assets/img/publication_preview/oodcv_v2-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/oodcv_v2.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="oodcv_v2.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="zhao2023ood" class="col-sm-9"> <div class="title">OOD-CV-v2: An Extended Benchmark for Robustness to Out-of-Distribution Shifts of Individual Nuisances in Natural Images</div> <div class="author"> <a href="https://info.zhaobc.me" rel="external nofollow noopener" target="_blank">Bingchen Zhao</a> , <a href="https://jiahaoplus.github.io" rel="external nofollow noopener" target="_blank">Jiahao Wang</a>, <em>Wufei Ma</em>, <a href="https://artur.jesslen.ch" rel="external nofollow noopener" target="_blank">Artur Jesslen</a> , Siwei Yang, <a href="https://scholar.google.com/citations?user=Xg8NCKgAAAAJ" rel="external nofollow noopener" target="_blank">Shaozuo Yu</a>, Oliver Zendel, Christian Theobalt, <a href="https://www.cs.jhu.edu/~ayuille/" rel="external nofollow noopener" target="_blank">Alan Yuille</a>, and <a href="https://adamkortylewski.com" rel="external nofollow noopener" target="_blank">Adam Kortylewski</a> </div> <div class="periodical"> <em>IEEE Transactions on Pattern Analysis and Machine Intelligence</em>, 2024 </div> <div class="periodical"> </div> <div> <abbr class="badge rounded" style="background-color:#009f36"> Dataset </abbr> <abbr class="badge rounded" style="background-color:#00369f"> 3D Vision </abbr> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2304.10266" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="https://bzhao.me/OOD-CV/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="abstract hidden"> <p>We introduce OOD-CV-v2, a benchmark dataset that includes out-of-distribution examples of 10 object categories in terms of pose, shape, texture, context and the weather conditions, and enables benchmarking of models for image classification, object detection, and 3D pose estimation.</p> </div> </div> </div> </li> </ol> <h2 class="bibliography">2023</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-3 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/3dvqa-480.webp 480w,/assets/img/publication_preview/3dvqa-800.webp 800w,/assets/img/publication_preview/3dvqa-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/3dvqa.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="3dvqa.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="wang20243d" class="col-sm-9"> <div class="title">3d-aware visual question answering about parts, poses and occlusions</div> <div class="author"> <a href="https://xingruiwang.github.io" rel="external nofollow noopener" target="_blank">Xingrui Wang</a>, <em>Wufei Ma</em> , <a href="https://lizw14.github.io" rel="external nofollow noopener" target="_blank">Zhuowan Li</a>, <a href="https://adamkortylewski.com" rel="external nofollow noopener" target="_blank">Adam Kortylewski</a>, and <a href="https://www.cs.jhu.edu/~ayuille/" rel="external nofollow noopener" target="_blank">Alan Yuille</a> </div> <div class="periodical"> <em>In Advances in Neural Information Processing Systems</em> , 2023 </div> <div class="periodical"> </div> <div> <abbr class="badge rounded" style="background-color:#009f36"> Dataset </abbr> <abbr class="badge rounded" style="background-color:#00369f"> 3D Vision </abbr> <abbr class="badge rounded" style="background-color:#b509ac"> Vision-Lanugage </abbr> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2310.17914" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="https://github.com/XingruiWang/3D-Aware-VQA" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>We introduce Super-CLEVR-3D, a compositional reasoning dataset that contains questions about object parts, their 3D poses, and occlusions. We propose PO3D-VQA, a 3D-aware VQA model that combines probabilistic neural symbolic program execution with 3D generative representations of objects.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-3 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/animal3d_sm-480.webp 480w,/assets/img/publication_preview/animal3d_sm-800.webp 800w,/assets/img/publication_preview/animal3d_sm-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/animal3d_sm.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="animal3d_sm.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="xu2023animal3d" class="col-sm-9"> <div class="title">Animal3D: A Comprehensive Dataset of 3D Animal Pose and Shape</div> <div class="author"> <a href="https://xujiacong.github.io" rel="external nofollow noopener" target="_blank">Jiacong Xu</a> , Yi Zhang, <a href="https://www.linkedin.com/in/jiawei-peng-59a713190/" rel="external nofollow noopener" target="_blank">Jiawei Peng</a>, <em>Wufei Ma</em>, <a href="https://artur.jesslen.ch" rel="external nofollow noopener" target="_blank">Artur Jesslen</a>, Pengliang Ji, Qixin Hu , Jiehua Zhang, <a href="https://qihao067.github.io" rel="external nofollow noopener" target="_blank">Qihao Liu</a> , <a href="https://jiahaoplus.github.io" rel="external nofollow noopener" target="_blank">Jiahao Wang</a>, and  others </div> <div class="periodical"> <em>In Proceedings of the IEEE/CVF International Conference on Computer Vision</em> , 2023 </div> <div class="periodical"> </div> <div> <abbr class="badge rounded" style="background-color:#009f36"> Dataset </abbr> <abbr class="badge rounded" style="background-color:#00369f"> 3D Vision </abbr> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2308.11737" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="https://xujiacong.github.io/Animal3D/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://drive.google.com/drive/folders/17KRe8Z7jCZNDeBu45Wx2zS8Yh2tV_t2v?usp=sharing" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Data</a> </div> <div class="abstract hidden"> <p>Animal3D consists of 3379 images collected from 40 mammal species, high-quality annotations of 26 keypoints, and importantly the pose and shape parameters of the SMAL model.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-3 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/superclevr_eg-480.webp 480w,/assets/img/publication_preview/superclevr_eg-800.webp 800w,/assets/img/publication_preview/superclevr_eg-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/superclevr_eg.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="superclevr_eg.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="li2023super" class="col-sm-9"> <div class="title">Super-CLEVR: A Virtual Benchmark to Diagnose Domain Robustness in Visual Reasoning</div> <div class="author"> <a href="https://lizw14.github.io" rel="external nofollow noopener" target="_blank">Zhuowan Li</a> , <a href="https://xingruiwang.github.io" rel="external nofollow noopener" target="_blank">Xingrui Wang</a>, Elias Stengel-Eskin, <a href="https://adamkortylewski.com" rel="external nofollow noopener" target="_blank">Adam Kortylewski</a>, <em>Wufei Ma</em>, Benjamin Van Durme, and <a href="https://www.cs.jhu.edu/~ayuille/" rel="external nofollow noopener" target="_blank">Alan Yuille</a> </div> <div class="periodical"> <em>In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em> , 2023 </div> <div class="periodical"> </div> <div class="periodical"> <span style="color:#e74c3c"><b>(Highlight)</b></span> </div> <div> <abbr class="badge rounded" style="background-color:#009f36"> Dataset </abbr> <abbr class="badge rounded" style="background-color:#b509ac"> Vision-Lanugage </abbr> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2212.00259" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="https://github.com/Lizw14/Super-CLEVR" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>We introduce Super-CLEVR, where different factors in VQA domain shifts can be isolated in order that their effects can be studied independently. We propose probabilistic NSVQA (P-NSVQA), which extends NSVQA with uncertainty reasoning.</p> </div> </div> </div> </li> </ol> <h2 class="bibliography">2022</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-3 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/eccv22_6dpose_small-480.webp 480w,/assets/img/publication_preview/eccv22_6dpose_small-800.webp 800w,/assets/img/publication_preview/eccv22_6dpose_small-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/eccv22_6dpose_small.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="eccv22_6dpose_small.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="ma2022robust" class="col-sm-9"> <div class="title">Robust Category-Level 6D Pose Estimation with Coarse-to-Fine Rendering of Neural Features</div> <div class="author"> <em>Wufei Ma</em> , <a href="https://angtianwang.github.io" rel="external nofollow noopener" target="_blank">Angtian Wang</a>, <a href="https://www.cs.jhu.edu/~ayuille/" rel="external nofollow noopener" target="_blank">Alan Yuille</a>, and <a href="https://adamkortylewski.com" rel="external nofollow noopener" target="_blank">Adam Kortylewski</a> </div> <div class="periodical"> <em>In European Conference on Computer Vision</em> , 2022 </div> <div class="periodical"> </div> <div> <abbr class="badge rounded" style="background-color:#00369f"> 3D Vision </abbr> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2209.05624" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="https://github.com/wufeim/6d_pose_eccv22" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>We introduce a coarse-to-fine optimization strategy that utilizes the neural features to estimate a sparse set of 6D object proposals, which are subsequently refined with gradient-based optimization.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-3 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/robin2-480.webp 480w,/assets/img/publication_preview/robin2-800.webp 800w,/assets/img/publication_preview/robin2-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/robin2.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="robin2.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="zhao2022ood" class="col-sm-9"> <div class="title">OOD-CV: A Benchmark for Robustness to Out-of-Distribution Shifts of Individual Nuisances in Natural Images</div> <div class="author"> <a href="https://info.zhaobc.me" rel="external nofollow noopener" target="_blank">Bingchen Zhao</a>, <a href="https://scholar.google.com/citations?user=Xg8NCKgAAAAJ" rel="external nofollow noopener" target="_blank">Shaozuo Yu</a>, <em>Wufei Ma</em> , Mingxin Yu, Shenxiao Mei , <a href="https://angtianwang.github.io" rel="external nofollow noopener" target="_blank">Angtian Wang</a>, <a href="https://tacju.github.io" rel="external nofollow noopener" target="_blank">Ju He</a>, <a href="https://www.cs.jhu.edu/~ayuille/" rel="external nofollow noopener" target="_blank">Alan Yuille</a>, and <a href="https://adamkortylewski.com" rel="external nofollow noopener" target="_blank">Adam Kortylewski</a> </div> <div class="periodical"> <em>In European conference on computer vision</em> , 2022 </div> <div class="periodical"> </div> <div class="periodical"> <span style="color:#e74c3c"><b>(Oral)</b></span> </div> <div> <abbr class="badge rounded" style="background-color:#009f36"> Dataset </abbr> <abbr class="badge rounded" style="background-color:#00369f"> 3D Vision </abbr> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2111.14341" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> </div> <div class="abstract hidden"> <p>We introduce OOD-CV, a benchmark dataset for diagnosing the robustness of 2D and 3D vision algorithms to individual nuisances in real-world images.</p> </div> </div> </div> </li> </ol> <h2 class="bibliography">2021</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-3 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/pub-3-480.webp 480w,/assets/img/publication_preview/pub-3-800.webp 800w,/assets/img/publication_preview/pub-3-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/pub-3.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="pub-3.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="mohsin2021making" class="col-sm-9"> <div class="title">Making group decisions from natural language-based preferences</div> <div class="author"> Farhad Mohsin, Lei Luo, <em>Wufei Ma</em>, Inwon Kang , Zhibing Zhao , Ao Liu, Rohit Vaish, and Lirong Xia </div> <div class="periodical"> <em>In Proceedings of the 8th International Workshop on Computational Social Choice (COMSOC)</em> , 2021 </div> <div class="periodical"> </div> <div> <abbr class="badge rounded" style="background-color:#009f36"> Dataset </abbr> <abbr class="badge rounded" style="background-color:#3948db"> Preference Learning </abbr> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="/assets/pdf/PT_Paper_78.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://github.com/farhadmohsin/CollegeConfidentialComparativeDisucssions" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Data</a> <a href="" class="btn btn-sm z-depth-0" role="button">Journal</a> </div> <div class="abstract hidden"> <p>We propose a framework for making group decisions from natural language-based preferences. Experiments on the real world data confirms the efficacy of our method.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-3 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/material_perspective-480.webp 480w,/assets/img/publication_preview/material_perspective-800.webp 800w,/assets/img/publication_preview/material_perspective-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/material_perspective.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="material_perspective.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="baskaran2021adoption" class="col-sm-9"> <div class="title">Adoption of Image-Driven Machine Learning for Microstructure Characterization and Materials Design: A Perspective</div> <div class="author"> <a href="https://scholar.google.com/citations?user=hbCR5qgAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Arun Baskaran</a>, <a href="https://ne.ncsu.edu/people/ekautz/" rel="external nofollow noopener" target="_blank">Elizabeth J Kautz</a>, Aritra Chowdhary, <em>Wufei Ma</em>, <a href="https://www.cs.rpi.edu/~yener/" rel="external nofollow noopener" target="_blank">Bulent Yener</a>, and Daniel Lewis </div> <div class="periodical"> <em>JOM</em>, 2021 </div> <div class="periodical"> </div> <div> <abbr class="badge rounded" style="background-color:#eb984e"> Microstructure </abbr> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2105.09729" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="" class="btn btn-sm z-depth-0" role="button">Journal</a> </div> <div class="abstract hidden"> <p>We first review the application of image-driven machine learning approaches to the field of materials characterization. Then we analyze and discuss the impact of various approaches at each step of the experiments.</p> </div> </div> </div> </li> </ol> <h2 class="bibliography">2020</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-3 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/pub-2-480.webp 480w,/assets/img/publication_preview/pub-2-800.webp 800w,/assets/img/publication_preview/pub-2-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/pub-2.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="pub-2.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="ma2020image" class="col-sm-9"> <div class="title">Image-driven discriminative and generative machine learning algorithms for establishing microstructure–processing relationships</div> <div class="author"> <em>Wufei Ma</em>, <a href="https://ne.ncsu.edu/people/ekautz/" rel="external nofollow noopener" target="_blank">Elizabeth J Kautz</a>, <a href="https://scholar.google.com/citations?user=hbCR5qgAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Arun Baskaran</a>, <a href="https://www.linkedin.com/in/aritrachowdhury1/" rel="external nofollow noopener" target="_blank">Aritra Chowdhury</a>, Vineet Joshi, <a href="https://www.cs.rpi.edu/~yener/" rel="external nofollow noopener" target="_blank">Bulent Yener</a>, and Daniel Lewis </div> <div class="periodical"> <em>Journal of Applied Physics</em>, 2020 </div> <div class="periodical"> </div> <div> <abbr class="badge rounded" style="background-color:#eb984e"> Microstructure </abbr> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2007.13417" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="https://wufeim.github.io/microstructure-characterization-II/" class="btn btn-sm z-depth-0" role="button">HTML</a> <a href="https://github.com/wufeim/microstructure-characterization-II" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="" class="btn btn-sm z-depth-0" role="button">Journal</a> </div> <div class="abstract hidden"> <p>Characterize 10 different microstructure representations with image texture features and quantitative metrics from image segmentation. For the microstructure generation task, two schemes are considered: 1) generating high-resolution (1024x1024) microstructure images from random noise; and 2) train a style transfer GAN for image generation conditioned on the segmentation label.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-3 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/pub-1-480.webp 480w,/assets/img/publication_preview/pub-1-800.webp 800w,/assets/img/publication_preview/pub-1-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/pub-1.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="pub-1.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="kautz2020image" class="col-sm-9"> <div class="title">An image-driven machine learning approach to kinetic modeling of a discontinuous precipitation reaction</div> <div class="author"> <a href="https://ne.ncsu.edu/people/ekautz/" rel="external nofollow noopener" target="_blank">Elizabeth Kautz</a>, <em>Wufei Ma</em>, Saumyadeep Jana, Arun Devaraj, Vineet Joshi, <a href="https://www.cs.rpi.edu/~yener/" rel="external nofollow noopener" target="_blank">Bülent Yener</a>, and Daniel Lewis </div> <div class="periodical"> <em>Materials Characterization</em>, 2020 </div> <div class="periodical"> </div> <div> <abbr class="badge rounded" style="background-color:#eb984e"> Microstructure </abbr> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/1906.05496" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="https://github.com/wufeim/microstructure-characterization" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="" class="btn btn-sm z-depth-0" role="button">Journal</a> </div> <div class="abstract hidden"> <p>Kinetic modeling of a discontinuous precipitation reaction (5 phases) by 1) deep learning with CNN, and 2) performing image segmentation of various microstructure and quantizing the area fractions.</p> </div> </div> </div> </li> </ol> </div> </article> </div> </div> <footer class="sticky-bottom mt-5" role="contentinfo"> <div class="container"> © Copyright 2025 Wufei Ma. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with theme modified from <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.min.js" integrity="sha256-rjmgmaB99riUNcdlrDtcAiwtLIojSxNyUFdl+Qh+rB4=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=UA-158881522-2"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","UA-158881522-2");</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script src="/assets/js/shortcut-key.js"></script> </body> </html>